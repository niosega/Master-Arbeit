%************************************************
\chapter{Parallelism}\label{ch:Parallelism}
%************************************************
Nowadays, there are more and more cores and computational units in computers and clusters. The problem is that extracting parallelism from a source code is not that easy, so the automatic parallelisation of nowadays languages is a challenge. For example, decide automaticaly to run a code on a GPU or a CPU requires an in-depth knowledge of the code and of the architecture. We will see in this chapter what is parallelism and describe the current architectures.

\section{Definitions}
Parallelism is a kind of computation in which multiples computations are carried out in the same time. The main idea of parallelism is to extract from a big problem multiples smalls problems that can be solved in parallel. 

There are several types of parallelism :
\begin{itemize}
\item Bit-Level parallelism : BLP is based on the size of the processor word size. For instance, to compute an addition of two 32 bits integers, a 16 bits processors will need two cycles whereas a 32 bits processor will only need one.
\item Instruction-Level parallelism : ILP works, as its name suggests, at Instruction level. This means that the dependencies of the code will be analyzed to be able to execute multiples instructions in parallel. \todo{Faire example}. There are multiples optimisations in modern compilers that act at Instruction-Level : Register renaming, Static Expansion etc.
\item Data parallelism : DP is the art of splitting data to multiples computational units, let them compute intermediate results and reconstruct the final results from the intermediate results. \todo{example sum tableau}
\item Task parallelism : TP is the art of distributing differents tasks (handle graphical interface, compute price of flight ticket) on multiples computational units. For instance, if you read this document on your computer, there are probably one thread for your favorite pdf reader and one for your system. 
\end{itemize}

Parallelism can be found at two stages :
\begin{itemize}
\item Software : Even if the most common languages were created for non-parallel architectures, more and more library or compilers options are developped to enhance parallelism during compilation/execution. Some implicitly parallel languages exist (SequenceL \todo{lien}) but are not widely used.
\item Hardware : Some architecture are better for few kind of computation. For example, GPU behave well with array operations. The architecture (memory types and placement, type of nodes and computational units) allows better performance and parallelism  for only a restrict number of cases. Parallelism is the art of building a system that fit to our needs.
\end{itemize}

\section{Architectures}
In this section, we will describe the main architectures that are available to execute programs. The principle, advantages and disavantadges of each will be presented.

\subsection{CPU}
A Central Processing Unit (CPU) is the part of computers that do the computations. This is the most common type of computational units, everyone has one in its machine. The market is flooded  by Intel\todo{lien} and AMD\todo{lien}. A processor can have one or multiple cores (nowadays 2 to 8), each core is an independant computational unit but the memory is shared between all of them. 
\todo{dessin cpu}.

A CPU executes a flow of instructions according to a fixed pipeline. The most simple pipeline is Instruction Fetch, Instruction Decode, Execute, Write Back.
\begin{itemize}
\item Fetch : The instruction is loaded from the main memory.
\item Decode : The instruction is decoded inside the processor, ie the processor understands what it has to compute.
\item Execute : The computation is done.
\item Write Back : The result is written back in memory.
\end{itemize}

\todo{dessin pipeline}

To enhance the performance of a program, some optimisations can be done at Pipeline-Level. For example, speculative execution allows the processor to load instruction before the end of others running instructions. \todo{faire example de boucle qui charge la dereniere iteration alors qu'il fallait pas}

The advantage of a CPU is that it is possible to design instructions for every specific needs : there are some instructions only used for cryptography in our modern processor. The disavantadge is that the circuit become bigger and bigger and so slower and slower. A CPU follows a single instruction single data  schema : this means that one instruction operate on one data. Performance on vector operations are bad that is why modern processors have Single Instruction Multiple Datas (SIMD) instructions that can perform the same operations on multiple data (doing +2 on all cells of an array for example).

\subsection{GPU}

\subsection{Others architectures}
ManyCore are processors that contains multiples (10 to 1000) simpler and independant cores. They can be seen as a small cluster inside a chip. This means that programming models like OpenMP or MPI can be used to compiled a code to a ManyCore target. Sunway TaihuLight, a chinese supercomputer is build from 40960 homemade manycores.

Application-Specific Integrated Circuit (ASIC) are processors that are designed for a specific use cases and are not reprogrammable. For instance, if a system need an efficient circuit that compute Fourrier transform, this can be implemented in a ASIC. The advantage is that, due to the fact that the ASIC is designed for a specific use, the performance will be better than with a CPU.

Field-Programmable Gate Array (FPGA) are between ASIC and CPU. FPGA are circuit that can be reconfigured multiples times but one configured can only do one things. For instance, some prototypes chips have a CPU and a FPGA so that if the compiler detects a piece of code that are run multiple time, it will configure the FPGA to do this operation and let the CPU ask the FPGA each time it needs the value.  

\section{Dependencies analysis}